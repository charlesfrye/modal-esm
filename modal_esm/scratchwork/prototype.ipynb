{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import py3Dmol\n",
    "\n",
    "from esm.pretrained import ESM3_sm_open_v0\n",
    "from esm.utils.structure.protein_chain import ProteinChain\n",
    "from esm.utils.structure.protein_structure import compute_affine_and_rmsd\n",
    "from esm.utils.structure.aligner import Aligner\n",
    "from esm.sdk.api import ESMProtein, GenerationConfig\n",
    "from esm.tokenization import EsmSequenceTokenizer\n",
    "from esm.utils.constants.esm3 import SEQUENCE_MASK_TOKEN\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.utils.generation import iterative_sampling_raw\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig, SamplingConfig, SamplingTrackConfig\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from transformers import EsmTokenizer, EsmForSequenceClassification\n",
    "import torch\n",
    "from peft import PeftModelForSequenceClassification\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%set_env TOKENIZERS_PARALLELISM=false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pad_coords(coords, start_pad, end_pad):\n",
    "    return torch.cat([\n",
    "        torch.full((start_pad, 37, 3), float('nan')), \n",
    "        coords, \n",
    "        torch.full((end_pad, 37, 3), float('nan'))\n",
    "        ], dim=0)\n",
    "    \n",
    "def mask_protein_and_sequence(\n",
    "    sequence: str, \n",
    "    coords: torch.Tensor, \n",
    "    sequence_fixed_residue_indices: List[int], \n",
    "    structure_fixed_residue_indices: List[int],\n",
    "    mask_fraction: float,\n",
    "    ) -> Tuple[str, torch.Tensor]:\n",
    "    \n",
    "    num_residues = len(sequence)\n",
    "    num_to_mask = int(num_residues * mask_fraction)\n",
    "    all_indices = list(range(num_residues))\n",
    "    mask_indices = np.random.choice(all_indices, num_to_mask, replace=False)\n",
    "    \n",
    "    masked_sequence = list(sequence)\n",
    "    masked_coords = coords.clone()\n",
    "    \n",
    "    for idx in mask_indices:\n",
    "        masked_sequence[idx] = '_'\n",
    "        masked_coords[idx, :, :] = torch.full_like(masked_coords[idx, :, :], float('nan'))\n",
    "        \n",
    "    for idx in sequence_fixed_residue_indices:\n",
    "        masked_sequence[idx] = sequence[idx]\n",
    "        \n",
    "    for idx in structure_fixed_residue_indices:\n",
    "        masked_coords[idx, :, :] = coords[idx, :, :]\n",
    "    \n",
    "    return ''.join(masked_sequence), masked_coords\n",
    "\n",
    "def sequence_diff(ref, query):\n",
    "    try:\n",
    "        diff = []\n",
    "        for i in range(len(ref)):\n",
    "            if ref[i] != query[i]:\n",
    "                diff.append(f\"{ref[i]}{i}{query[i]}\")\n",
    "        return '/'.join(diff)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the ESM3 model\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(\"cuda\") \n",
    "tokenizer = EsmSequenceTokenizer()\n",
    "\n",
    "\n",
    "# set up saprot models\n",
    "yfp_adapter_input = \"SaProtHub/Model-EYFP-650M\"\n",
    "base_model_name = \"westlake-repl/SaProt_650M_AF2\"\n",
    "fluor_adapter_input = 'SaProtHub/Model-Fluorescence-650M'\n",
    "\n",
    "yfp_adapter_path = snapshot_download(repo_id=yfp_adapter_input, repo_type=\"model\")\n",
    "fluor_adapter_path = snapshot_download(repo_id=fluor_adapter_input, repo_type=\"model\")\n",
    "base_model = EsmForSequenceClassification.from_pretrained(base_model_name, num_labels=1,)\n",
    "saprot_yfp_model = PeftModelForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    yfp_adapter_path,\n",
    ")\n",
    "\n",
    "base_model_fluor = EsmForSequenceClassification.from_pretrained(base_model_name, num_labels=1,)\n",
    "saprot_fluor_model = PeftModelForSequenceClassification.from_pretrained(\n",
    "    base_model_fluor,\n",
    "    fluor_adapter_path,\n",
    ")\n",
    "\n",
    "tokenizer = EsmTokenizer.from_pretrained(base_model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "saprot_yfp_model.to(device);\n",
    "saprot_fluor_model.to(device);\n",
    "\n",
    "\n",
    "\n",
    "# set up prompts\n",
    "\n",
    "#eYFP\n",
    "ref_sequence = 'MVSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFGYGLQCFARYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSYQSALSKDPNEKRDHMVLLEFVTAAGITLGMDELYK'\n",
    "\n",
    "# Load YFP template\n",
    "pdb_id = \"1YFP\"  # Enhanced Yellow Fluorescent Protein\n",
    "# pdb_id = \"1QY3\"\n",
    "chain_id = \"A\"\n",
    "ref_chain = ProteinChain.from_rcsb(pdb_id, chain_id)\n",
    "\n",
    "ref_pdb_start = ref_chain.residue_index[0] # pad this much at the front\n",
    "ref_pdb_end = ref_chain.residue_index[-1]\n",
    "end_padding = len(ref_sequence) - ref_pdb_end + 2\n",
    "\n",
    "coords = torch.tensor(ref_chain.atom37_positions)\n",
    "ref_sequence_beginning = ref_sequence[:ref_pdb_start]\n",
    "ref_sequence_ending = ref_sequence[ref_pdb_end:]\n",
    "padded_coords = pad_coords(coords, ref_pdb_start, end_padding)\n",
    "padded_ref_chain = ProteinChain.from_atom37(padded_coords)\n",
    "\n",
    "\n",
    "# fixed positions\n",
    "positions = [1, 62, 65, 66, 67, 96, 222]\n",
    "sequence_fixed_indices = [position for position in positions]\n",
    "structure_fixed_indices = list(range(58, 72)) + [96, 222]\n",
    "\n",
    "\n",
    "# masking config\n",
    "mask_fraction = 0.025  # Define the fraction of non-preserved residues to mask\n",
    "num_to_mask = int(len(ref_sequence) * mask_fraction)\n",
    "\n",
    "\n",
    "sequence_prompt, structure_prompt = mask_protein_and_sequence(\n",
    "    ref_sequence, \n",
    "    padded_coords, \n",
    "    sequence_fixed_indices, \n",
    "    structure_fixed_indices,\n",
    "    mask_fraction\n",
    "    )\n",
    "\n",
    "prompt = ESMProtein(\n",
    "    sequence=sequence_prompt, \n",
    "    coordinates=structure_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "def calculate_sequence_identity(seq1: str, seq2: str) -> float:\n",
    "    assert len(seq1) == len(seq2), \"Sequences must be of equal length\"\n",
    "    identical = sum(a == b for a, b in zip(seq1, seq2))\n",
    "    return identical / len(seq1)\n",
    "\n",
    "def calculate_rmsd(coords1: np.ndarray, coords2: np.ndarray) -> float:\n",
    "    diff = coords1 - coords2\n",
    "    return np.sqrt(np.mean(np.sum(diff**2, axis=1)))\n",
    "\n",
    "def calculate_template_rmsd(variant: ProteinChain, template: ProteinChain, residues: List[int]) -> float:\n",
    "    aligner = Aligner(variant, template)\n",
    "    aligned_variant = aligner.apply(variant)\n",
    "    return aligner.rmsd\n",
    "\n",
    "def calculate_pseudo_perplexity(model: ESM3_sm_open_v0, sequence: str) -> float:\n",
    "    tokenizer = EsmSequenceTokenizer()\n",
    "    tokens = torch.tensor([tokenizer.encode(sequence)]).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(sequence_tokens=tokens)\n",
    "    log_probs = torch.log_softmax(output.sequence_logits, dim=-1)\n",
    "    token_log_probs = log_probs[0, torch.arange(len(sequence)), tokens[0, 1:-1]]\n",
    "    return torch.exp(-token_log_probs.mean()).item()\n",
    "\n",
    "def calculate_n_gram_score(sequence: str, n: int = 3) -> float:\n",
    "    # This is a simplified version. For a full implementation, you'd need a background distribution.\n",
    "    from collections import Counter\n",
    "    ngrams = [sequence[i:i+n] for i in range(len(sequence)-n+1)]\n",
    "    counts = Counter(ngrams)\n",
    "    return -sum(count * np.log(count/len(ngrams)) for count in counts.values()) / len(ngrams)\n",
    "\n",
    "def calculate_pssm_score(sequence: str, pssm: np.ndarray) -> float:\n",
    "    # This is a placeholder. You'd need to implement or import a proper PSSM for YFP.\n",
    "    return 0.0\n",
    "\n",
    "def calculate_n_terminus_coil_count(chain: ProteinChain, n: int = 12) -> int:\n",
    "    ss = chain.dssp()[:n]\n",
    "    return sum(1 for s in ss if s in ['S', 'T', 'C'])\n",
    "num_variants = 5000\n",
    "sequence_gen_configs = [GenerationConfig(\n",
    "    track=\"sequence\", \n",
    "    num_steps=num_to_mask // 1, \n",
    "    # temperature=0.1\n",
    "    ) for _ in range(num_variants)]\n",
    "\n",
    "structure_gen_configs = [GenerationConfig(\n",
    "    track=\"structure\", \n",
    "    num_steps=num_to_mask // 1, \n",
    "    # temperature=0.1\n",
    "    ) for _ in range(num_variants)]\n",
    "\n",
    "sequence_prompts = [prompt for _ in range(num_variants)]\n",
    "\n",
    "# esm generation loop\n",
    "variants = []\n",
    "for _ in range(num_variants):\n",
    "    # Generate YFP variant sequence\n",
    "    sequence_generation_config = GenerationConfig(\n",
    "        track=\"sequence\", \n",
    "        num_steps=num_to_mask // 1, \n",
    "        # temperature=0.1\n",
    "        )\n",
    "    variant_sequence = model.generate(prompt, sequence_generation_config)\n",
    "\n",
    "\n",
    "    # Generate structure for the YFP variant\n",
    "    structure_generation_config = GenerationConfig(\n",
    "        track=\"structure\", \n",
    "        num_steps=num_to_mask // 1, \n",
    "        # temperature=0.1\n",
    "        )\n",
    "    variant = model.generate(\n",
    "        variant_sequence, \n",
    "        structure_generation_config\n",
    "        )\n",
    "\n",
    "    # Convert ESMProtein to ProteinChain for easier handling\n",
    "    variant_chain = variant.to_protein_chain()\n",
    "    \n",
    "    variants.append(variant_chain)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a list to store metrics for each variant\n",
    "esm_outputs = []\n",
    "\n",
    "# Name variants and include their AA sequence in the metrics_df\n",
    "for ii, variant in enumerate(variants):\n",
    "    # Calculate metrics\n",
    "    seq_identity = calculate_sequence_identity(variant.sequence, ref_sequence)\n",
    "    chromophore_rmsd = calculate_template_rmsd(variant, padded_ref_chain, [64, 65, 66])  # 0-indexed\n",
    "    template_helix_rmsd = calculate_template_rmsd(variant, padded_ref_chain, list(range(57, 71)))  # 0-indexed\n",
    "    pseudo_perplexity = calculate_pseudo_perplexity(model, variant.sequence)\n",
    "    n_gram_score = calculate_n_gram_score(variant.sequence)\n",
    "    # n_terminus_coil_count = calculate_n_terminus_coil_count(variant)\n",
    "    \n",
    "    # Append metrics to the list\n",
    "    esm_outputs.append({\n",
    "        \"name\": f'variant_{ii}',\n",
    "        \"sequence\": variant.sequence,\n",
    "        \"seq_identity\": seq_identity,\n",
    "        \"chromophore_rmsd\": chromophore_rmsd,\n",
    "        \"template_helix_rmsd\": template_helix_rmsd,\n",
    "        \"pseudo_perplexity\": pseudo_perplexity,\n",
    "        \"n_gram_score\": n_gram_score,\n",
    "        # \"n_terminus_coil_count\": n_terminus_coil_count\n",
    "    })\n",
    "\n",
    "# Convert the list of metrics to a DataFrame\n",
    "esm_outputs_df = pd.DataFrame(esm_outputs)\n",
    "esm_outputs_df\n",
    "\n",
    "\n",
    "\n",
    "def AA_to_SA(aa_seq):\n",
    "    sa_seq = ''\n",
    "    for aa in aa_seq:\n",
    "        sa_seq += aa + '#'\n",
    "    return sa_seq\n",
    "\n",
    "\n",
    "\n",
    "aa_seqs = [\n",
    "    {'name': 'eYFP', 'sequence': 'MVSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFGYGLQCFARYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSYQSALSKDPNEKRDHMVLLEFVTAAGITLGMDELYK',},\n",
    "    {'name': 'Citrine', 'sequence': 'MVSKGEELFT GVVPILVELD GDVNGHKFSV SGEGEGDATY GKLTLKFICT TGKLPVPWPT LVTTFGYGLM CFARYPDHMK QHDFFKSAMP EGYVQERTIF FKDDGNYKTR AEVKFEGDTL VNRIELKGID FKEDGNILGH KLEYNYNSHN VYIMADKQKN GIKVNFKIRH NIEDGSVQLA DHYQQNTPIG DGPVLLPDNH YLSYQSALSK DPNEKRDHMV LLEFVTAAGI TLGMDELYK'.replace(' ', '')},\n",
    "    {'name': 'mCitrine', 'sequence': 'MVSKGEELFT GVVPILVELD GDVNGHKFSV SGEGEGDATY GKLTLKFICT TGKLPVPWPT LVTTFGYGLM CFARYPDHMK QHDFFKSAMP EGYVQERTIF FKDDGNYKTR AEVKFEGDTL VNRIELKGID FKEDGNILGH KLEYNYNSHN VYIMADKQKN GIKVNFKIRH NIEDGSVQLA DHYQQNTPIG DGPVLLPDNH YLSYQSKLSK DPNEKRDHMV LLEFVTAAGI TLGMDELYK'.replace(' ', '')},\n",
    "    {'name': 'Citrine2', 'sequence': 'MVSKGEELFT GVVPILVELD GDVNGHKFSV TGEGEGDATY GKLTLKFICT TGKLPVPWPT LVTTFGYGLT CFARYPDHMK QHDFFKSAMP EGYVQERTIF FKDDGNYKTR AEVKFEGDTL VNRIELKGID FKEDGNILGH KLEYNHNSHY VYIMADKQKN GIKANFKIRH NIEDGSVQLA DHYQQNTPIG DGPVLLPDNH YLSYQSQLSK DPNEERDHTV LLEFVTAAGI TLGMGELYK'.replace(' ', '')},\n",
    "    {'name': 'Venus', 'sequence': 'MVSKGEELFT GVVPILVELD GDVNGHKFSV SGEGEGDATY GKLTLKLICT TGKLPVPWPT LVTTLGYGLQ CFARYPDHMK QHDFFKSAMP EGYVQERTIF FKDDGNYKTR AEVKFEGDTL VNRIELKGID FKEDGNILGH KLEYNYNSHN VYITADKQKN GIKANFKIRH NIEDGGVQLA DHYQQNTPIG DGPVLLPDNH YLSYQSALSK DPNEKRDHMV LLEFVTAAGI TLGMDELYK'.replace(' ', '')},\n",
    "    {'name': 'mVenus', 'sequence': 'MVSKGEELFT GVVPILVELD GDVNGHKFSV SGEGEGDATY GKLTLKLICT TGKLPVPWPT LVTTLGYGLQ CFARYPDHMK QHDFFKSAMP EGYVQERTIF FKDDGNYKTR AEVKFEGDTL VNRIELKGID FKEDGNILGH KLEYNYNSHN VYITADKQKN GIKANFKIRH NIEDGGVQLA DHYQQNTPIG DGPVLLPDNH YLSYQSKLSK DPNEKRDHMV LLEFVTAAGI TLGMDELYK'.replace(' ', '')},\n",
    "    {'name': 'mTurquoise', 'sequence': 'MVSKGEELFT GVVPILVELD GDVNGHKFSV SGEGEGDATY GKLTLKFICT TGKLPVPWPT LVTTLSWGVQ CFARYPDHMK QHDFFKSAMP EGYVQERTIF FKDDGNYKTR AEVKFEGDTL VNRIELKGID FKEDGNILGH KLEYNYISDN VYITADKQKN GIKANFKIRH NIEDGGVQLA DHYQQNTPIG DGPVLLPDNH YLSTQSKLSK DPNEKRDHMV LLEFVTAAGI TLGMDELYK'.replace(' ', '')},\n",
    "    {'name': 'mEmerald', 'sequence': 'MVSKGEELFT GVVPILVELD GDVNGHKFSV SGEGEGDATY GKLTLKFICT TGKLPVPWPT LVTTLTYGVQ CFARYPDHMK QHDFFKSAMP EGYVQERTIF FKDDGNYKTR AEVKFEGDTL VNRIELKGID FKEDGNILGH KLEYNYNSHK VYITADKQKN GIKVNFKTRH NIEDGSVQLA DHYQQNTPIG DGPVLLPDNH YLSTQSKLSK DPNEKRDHMV LLEFVTAAGI TLGMDELYK'.replace(' ', '')},\n",
    "    {'name': 'mRuby3', 'sequence': 'MVSKGEELIK ENMRMKVVME GSVNGHQFKC TGEGEGRPYE GVQTMRIKVI EGGPLPFAFD ILATSFMYGS RTFIKYPADI PDFFKQSFPE GFTWERVTRY EDGGVVTVTQ DTSLEDGELV YNVKVRGVNF PSNGPVMQKK TKGWEPNTEM MYPADGGLRG YTDIALKVDG GGHLHCNFVT TYRSKKTVGN IKMPGVHAVD HRLERIEESD NETYVVQREV AVAKYSNLGG GMDELYK'.replace(' ', '')},\n",
    "    {'name': 'mStayGold2', 'sequence': 'MVSTGEELFT GVVPFKFQLK GTINGKSFTV EGEGEGNSHE GSHKGKYVCT SGKLPMSWAA LGTSFGYGMK YYTKYPSGLK NWFHEVMPEG FTYDRHIQYK GDGSIHAKHQ HFMKNGTYHN IVEFTGQDFK ENSPVLTGDM DVSLPNEVQH IPRDDGVECT VTLTYPLLSD ESKCVEAYQN TIIKPLHNQP APDVPYHWIR KQYTQSKDDT EERDHIIQSE TLEAHLYSRT KLE'.replace(' ', '')},\n",
    "    ]\n",
    "\n",
    "\n",
    "aa_seqs += esm_outputs_df[['name', 'sequence']].to_dict(orient='records')\n",
    "\n",
    "\n",
    "\n",
    "sa_seqs = [{'name': aa_seq['name'], 'sa_sequence': AA_to_SA(aa_seq['sequence']), 'length': len(aa_seq['sequence'])} for aa_seq in aa_seqs]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(sa_seqs)\n",
    "\n",
    "\n",
    "yfp_outputs_list = []\n",
    "fluor_outputs_list = []\n",
    "for index in tqdm(range(len(df))):\n",
    "    seq = df['sa_sequence'].iloc[index]\n",
    "    inputs = tokenizer(seq, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad(): yfp_outputs = saprot_yfp_model(**inputs)\n",
    "    yfp_outputs_list.append(float(yfp_outputs.logits.detach().cpu().numpy()[0][0]))\n",
    "    \n",
    "    with torch.no_grad(): fluor_outputs = saprot_fluor_model(**inputs)\n",
    "    fluor_outputs_list.append(float(fluor_outputs.logits.detach().cpu().numpy()[0][0]))\n",
    "    \n",
    "df['yfp_model_score'] = yfp_outputs_list\n",
    "df['fluor_model_score'] = fluor_outputs_list\n",
    "df = df.merge(esm_outputs_df, how='left', on='name')\n",
    "df['diff'] = df.apply(lambda row: sequence_diff(ref_sequence, row['sequence']), axis=1)\n",
    "df['n_mutations'] = df['diff'].apply(lambda x: len(x.split('/')) if x != '' else 0)\n",
    "df = df.sort_values('n_mutations', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.dropna(subset=['pseudo_perplexity']), vars=['yfp_model_score', 'fluor_model_score', 'chromophore_rmsd', 'template_helix_rmsd', 'pseudo_perplexity', 'n_gram_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# viz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize original YFP structure and generated variant\n",
    "def visualize_structures(template: ProteinChain, variant: ProteinChain, highlight_residues: List[int] = None):\n",
    "    view = py3Dmol.view(width=800, height=400, viewergrid=(1, 2))\n",
    "    \n",
    "    # Template structure\n",
    "    template_pdb = template.to_pdb_string()\n",
    "    view.addModel(template_pdb, \"pdb\", viewer=(0,0))\n",
    "    view.setStyle({\"cartoon\": {\"color\": \"lightgrey\"}}, viewer=(0,0))\n",
    "    if highlight_residues:\n",
    "        view.addStyle({\"resi\": highlight_residues}, {\"cartoon\": {\"color\": \"red\"}}, viewer=(0,0))\n",
    "    \n",
    "    # Variant structure\n",
    "    variant_pdb = variant.to_pdb_string()\n",
    "    view.addModel(variant_pdb, \"pdb\", viewer=(0,1))\n",
    "    view.setStyle({\"cartoon\": {\"color\": \"lightblue\"}}, viewer=(0,1))\n",
    "    if highlight_residues:\n",
    "        view.addStyle({\"resi\": highlight_residues}, {\"cartoon\": {\"color\": \"red\"}}, viewer=(0,1))\n",
    "    \n",
    "    view.zoomTo()\n",
    "    return view\n",
    "\n",
    "print(\"Visualizing original YFP structure (left) and generated variant (right) with key residues highlighted in red:\")\n",
    "all_key_residues = list(set(sequence_fixed_indices + structure_fixed_indices))\n",
    "visualize_structures(ref_chain, variant_chain, all_key_residues).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modal-esm-j6xT_jFm-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
